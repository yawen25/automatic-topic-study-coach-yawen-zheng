title,description
b1,Word Frequency AnalysisText Data Mining For Business Decisions Module 4 Word Frequency Analysis How do we discover the most frequent words in text data?
b2,"Data Mining- Continuing Model-Makingbusiness understanding, data understanding, data preparation, modeling, evaluation, deployemet, data, regression, clustering, association, anomaly detection, time series forecasting, text mining, feature selection, classification, data mining, term frequency, keyword analysis, sentiment analysis, similarity scoring"
b3,"Term Frequency Analysis is one of the Most Basic Text Mining Algorithmsterm frequency analysis, keyword analysis, sentiment analysis, visualizing text data, coding qualitative data, named entity recognition, topic recognition, transformers, text data mining"
b4,"How do we count words?• Counting words is the most basic technique in text analysis. – The starting point for most investigations. It is called lexical analysis • Assume that the most frequently-appearing words hold some meaning: they are somewhat more important that other words. – We tabulate their frequency because they likely have some greater significance in the context of our text. – We ignore nuances, such as grammatical structure (for example, is the word a noun or a verb?) and how a term is used (Sarcasm? Irony? Fact?). – that’s called semantic analysis – We also know that not all words carry significant meaning, such as propositions or conjunctions (for example, “and,” “beside,” “at,” or “in.”) – We assume we can safely ignore these and remove them from the text using a list of words with less meaning (stopwords). – We strive to create a Bag-of-Words text data file (document) or text data field (data point in a cell of a spreadsheet). • We count the most meaningful words to determine which are more important and which are less important. • N-grams – how many words in a pattern of words are we associating and counting as a pattern • “No” and ”good” counted separately is a 1-word ngram, “no good” is a 2-word engram"
b5,"Why do we count words for business purposes?• We survey what customers wanted in a product. – Besidesaskingquantitativequestions(suchas“onascaleof1to5...”),wealsoaskopen-ended questions (such as “Is there anything else you want us to know...”). – Doingawordfrequencyanalysisofthetextresponsestotheopen-endedquestionand comparing it to the word frequency analysis of the product description tells us if we are meeting expectations. – Themoreoftenthewordfrequencytablesmatch,themorewearematchingthecustomer’s expectations. • Consider a company that launched a campaign to make employees more aware of the new company mission. – Aftersometimeandusinganemployeesurvey,weasktheopen-endedquestion:“Canyoutell us what you think the company mission statement is?” – Bydoingawordfrequencyanalysisofthemissionstatementandcomparingittothefrequency analysis of the employee’s responses, we can gauge how successful our awareness campaign has been."
b6,"The Bag-of-Words Textual Data Model• The Bag-of-Words model is a representation of textual data used in natural language processing (NLP) and information retrieval (IR). • In this model, text (such as a sentence or a document) – represented as the bag (multiset) of its words, – without regard to grammar and word order, – with the multiplicity of the words maintained (for later counting, for example). • One of the main problems that can limit this technique's efficacy is the existence of prepositions, pronouns, and articles in our text. – These words likely appear frequently in our text, but they lack information about the main characteristics and topics in our document. – These are removed in the process of creating a Bag-of-Words text data file or data element. Chapter 5 demonstrates techniques to remove these unwanted terms before analysis."
b7,"Converting a Text into a Bag-of-Wordstext, bag of words, term frequency"
b8,"Use of a Stop Word List• Not all words are of interest when looking for meaning, so we often remove the most common words in a stopword list. Words such as “an,” “it,” “me,” and “the” are all words that are usually found on such a list. • Once removed, looking at the frequency of occurrence of the remaining words could be very informative. • If, for example, we removed the stop words and analyzed the word frequency of a Little Red Riding Hood fable, we would have a very informative table (Figure 5.1). It gives us an excellent picture of the story in the fable."
b9,"COUNTIF – A Powerful Tool• Excel has several text processing tools you should have in your toolbox: – COUNTIF – counts number of cells in a range that contain a word – LEN – counts total characters in a cell – =IF(LEN(TRIM(B2))=0,0,LEN(TRIM(B2))-LEN(SUBSTITUTE(B2,"" "",""""))+1) • Counts the number of words in a cell – Excel has a limit of 32,700 characters in any one cell"
b10,Exercises• Open the Graduate Course Description file • STEP 1 – Aggregate the course descriptions • STEP 2 – Split cell into words • STEP 3 – Transpose and Clean • STEP 4 – Set up table for use of Pivot Table • STEP 5 – Count unique words using a Pivot Table • STEP 6 – Remove stop words • STEP 7 – Final Term Frequency Table • SolutionfilehavethewordSOLUTIONinthefiletitle
b11,Voyant on the Web
b12,"Term Frequency using Voyant• IntheTF–CountWordsExcelfileselecttheaggregated top 7 course descriptions (10,000 characters) • Copytheaggregatedcoursedescriptionsintothe computer buffer • OpenVoyantontheweb:https://voyant-tools.org/ • PastetheaggregateddescriptionsastextintoVoyant and click Reveal • Inthetools selectGridToolsandTerms • BackintheselectionschooseExport and(text) • ScapetableandpasteintoExcel–compare"
b13,"Additional Exercises• Select a term to be removed via the stopword list, – edit the list remove “based” – run the analysis again and assure yourself that the term is no longer in the analysis • Run the analysis again by uploading the text file of the 7 courses into Voyant – Compare to the table from pasting the text directly into Voyant • Run the analysis again by creating a corpus of each of the 7 courses as individual text files – Use Document Terms Grid tool in Voyant – Download the table of term frequency by text and using a pivot table get a crosstabulation of term frequency by course"